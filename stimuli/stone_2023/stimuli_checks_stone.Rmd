---
title: "Check the experimental stimuli"
author: "Miriam Schulz"
date: "2025-02-19"
output:
  html_document:
    number_sections: true
    toc: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries, message=FALSE}
library("tidyverse")
library("ggplot2")
library("Rmisc")
library(stringr)
library(rlang)
library(corrplot)

rm(list = ls())
```


# About

This script checks the Stone et al. 2023 experimental stimuli and adds + analyzes metrics like similarity scores between the target words and target noun surprisal by 4 different LLMs.


# Inspect cloze probability per condition

Read the stimuli, filter for the relevant conditions and check the number of items:

```{r read-stimuli}
# Read in the data
dat <- read.csv("experimental_stimuli.tsv", header=TRUE, sep = "\t")

dat <- dat %>% 
  filter(cond_label %in% c("a", "b"))

# How many items are there in total?
length(unique(dat$item))
```

Check the distribution of Cloze probabilities:

```{r cloze-by-cond}
# Range of cloze values in each condition
dat %>% 
  dplyr::group_by(cond_label) %>% 
  dplyr::summarise(MeanCloze = mean(cloze.target),
                   ClozeSD = sd(cloze.target),
                   MinCloze = min(cloze.target),
                   MaxCloze = max(cloze.target))

# dat %>% summarySE(measurevar="cloze.target_smth",
#                   groupvars = "cond_label")

# Histogram of cloze probs
ggplot(data=dat,
       aes(cloze.target)) + 
  geom_histogram(bins = 10, fill = "steelblue", color = "steelblue4") + 
  labs(title = "Histogram of cloze probabilities") +
  theme_minimal()

# Density plot
ggplot(data=dat,
       aes(cloze.target, fill = cond_label, color = cond_label)) + 
  geom_density(alpha = 0.6) + 
  scale_fill_manual(values = c("navy", "magenta3")) +
  scale_color_manual(values = c("navy", "magenta3"), guide = "none") +
  labs(title = "Cloze probabilities by condition",
       fill = "Condition") +
  theme_minimal()
```

# Target length and frequency by condition

First remove punctuation from the noun (store the original in a different column).

```{r remove-punctuation}
dat$noun_punctuation <- dat$noun
dat$noun <- gsub("[[:punct:]]", "", dat$noun)
```

First, add target word length in n characters:

```{r add-length-freq}
dat <- dat %>%
  mutate(target_length = nchar(noun))  # add target word length
```

Then add the German Subtlex frequencies (TODO: check if ZipfSUBTLEX is the right measure):

```{r get-subtlex-freqs}
subtlex <- read.csv("subtlex-de.csv", header = TRUE, sep = ";")
subtlex <- subtlex %>% 
  select(Word, ZipfSUBTLEX) %>% 
  dplyr::rename(noun = Word)
subtlex <- subtlex %>%
  mutate(ZipfSUBTLEX = gsub(",", ".", ZipfSUBTLEX))
subtlex$ZipfSUBTLEX <- as.numeric(subtlex$ZipfSUBTLEX)

dat <- merge(dat, subtlex, by = c("noun"), all.x = TRUE, rm.na = FALSE)
dat <- dat[, c(2:10, 1, 11:19, 21, 20, 22)]
dat <- dat %>% 
  arrange(item, cond_label)
```

Check the resulting by-condition means:

```{r check-cloze-freq-length}
dat %>% 
  dplyr::group_by(cond_label) %>% 
  dplyr::summarise(MeanCloze = mean(cloze.target),
                   MeanLength = mean(target_length),
                   MeanFreq = mean(as.numeric(ZipfSUBTLEX), na.rm = TRUE))
                   #MinCloze = min(cloze.target),
                   #MaxCloze = max(cloze.target),
                   #ClozeSD = sd(cloze.target))
```

A NOTE OF CAUTION: Many frequencies could not be found in the German Subtlex corpus, therefore the above frequency estimate is to be treated with caution.

Missing frequency observations by condition (these might contain the same noun multiple times):

```{r freq-nas}
dat %>%
  dplyr::group_by(cond_label) %>%
  dplyr::summarize(na_count = sum(is.na(ZipfSUBTLEX)))
```


# Targets used more than once

```{r check-duplicate-nouns}
duplicates <- dat %>%
  dplyr::mutate(noun = gsub("[[:punct:]]", "", noun)) %>% 
  dplyr::group_by(noun) %>%
  dplyr::summarize(occurrences = n(),
            items = paste(unique(item), collapse = ", "),
            conditions = paste(unique(cond_label), collapse = ", ")) %>%
  dplyr::filter(occurrences > 1) %>% 
  arrange(-occurrences)

cat(nrow(duplicates), "nouns are used more than once in the stimuli.")

head(duplicates, n = 20)
```


# Similarity metrics

## Target-context similarity

Check Stone et al. context similarity (LSA):

```{r check-cosine-context}
dat %>% 
  dplyr::group_by(cond_label) %>% 
  dplyr::summarise(MeanSimilarity = mean(cosine_targetContext, na.rm = TRUE),
                   SimilaritySD = sd(cosine_targetContext, na.rm = TRUE),
                   MinSimilarity = min(cosine_targetContext, na.rm = TRUE),
                   MaxSimilarity = max(cosine_targetContext, na.rm = TRUE))
```

## Target-target similarity

Add FastText embeddings to check similarity between the two targets:

TODO


# Export for surprisal and embedding annotations

Export the preprocessed stimuli in the format required by the surprisal script.

```{r format-and-export}
dat <- dat %>%
  unite("sentence_target", context, sentence, pre_critical,
        determiner, adj, noun,
        sep = " ", remove = FALSE)

dat <- dat %>%
  mutate(sentence_target = str_replace_all(sentence_target,
                                           "[[:punct:]&&[^.]]", " "))

dat$sentence_target <- gsub("\\s+", " ", dat$sentence_target)
dat$sentence_target <- gsub("^\\s+|\\s+$", "", dat$sentence_target)

write.csv(dat, 'stimuli_stone_preprocessed.csv', row.names = FALSE,
          fileEncoding = "utf-8")

dat_for_embeddings <- dat %>% 
  select(c("item", "cond_label", "noun")) %>% 
  pivot_wider(names_from = cond_label, values_from = noun)  # wide format

write.csv(dat_for_embeddings, 'stimuli_for_embeddings.csv', row.names = FALSE)
```


# Embeddings

## Fasttext

Read embeddings and add to stimuli:

```{r read-embeddings}
# Read file
fasttext <- read.csv('stimuli_fasttext_embeddings.csv', header = TRUE)

# Transform to long format
fasttext_long <- fasttext %>%
  pivot_longer(cols = c(a, b), 
               names_to = "cond_label", 
               values_to = "noun")

# Merge
dat <- merge(dat, fasttext_long, by = c("item", "cond_label", "noun"))
```

Visualize/inspect the fasttext embeddings:

```{r check-embeddings}
# Histogram of fasttext embeddings
ggplot(data=fasttext,
       aes(fasttext)) + 
  geom_histogram(bins = 20, fill = "steelblue", color = "steelblue4") + 
  labs(title = "Histogram of fasttext target-target similarities") +
  theme_minimal()
```

Manually inspect which target noun pairs exhibit particularly high similarity:

```{r check-high-fasttext}
mean(fasttext$fasttext)
median(fasttext$fasttext)

fasttext_high <- fasttext %>% 
  filter(fasttext >= 0.5)

length(unique(fasttext_high$item))

fasttext_high
```

Result: 36 items exhibit particularly high fasttext similarity scores of $>=0.5$, among which many pairs corresponding to intuition, such as:

- Kissen-Polster
- Salbe-Creme
- Sack-Beutel

The highest similarity scores in the item set are observed for:

- Maßband-Lineal (0.69)
- Katze-Ratte (0.69)
- Zahnbürste-Zahnseide (0.68)
- Anzug-Sakko (0.67)
- Löffel-Teller (0.66)

The lowest similarity scores in the item set are observed for:

- Geschirrtuch-Mittel	(0.11)
- Gesangsbuch-Brötchen (0.11)
- Taschenrechner-Schieber (0.13)
- Fächer-Fön (0.13)
- Ladekabel-Teil (0.13)

To filter them out later, a high-fasttext flag is added to those items. 
In addition, the fasttext values are exported sorted by decreasing fasttext similarity score for manual inspection.

```{r flag-high-fasttext-export}
dat$fasttext_high <- ifelse(dat$item %in% fasttext_high$item, 1, 0)
fasttext <- fasttext %>% 
  arrange(-fasttext)
write.csv(fasttext, 'stimuli_fasttext_embeddings_decreasing.csv',
          row.names = FALSE)
```

Result of manual inspection: Even among the items with a similarity score of $<=0.5$, many seem critical.  
Manual inspection intuitively suggests to choose a cutoff value around $0.42$ or $0.40$, but even below this threshold value, some items might need to be manually excluded whose similarity could not be captured by fastText efficiently, such as 'Fußbilz-Erreger' ($0.397$) or 'Namensschild-Hemd' ($0.37$).

```{r check-high-fasttext-0.4}
fasttext_high <- fasttext %>% 
  filter(fasttext >= 0.4)
length(unique(fasttext_high$item))
```

Using a threshold of $>=0.40$ for exclusion would however result in the exclusion of 84 items.

TODO: check the GloVe embeddings as well and exclude stimuli which exceeded a threshold in either fastText or GloVe; perhaps this helps in detecting these items.


# Surprisal

The above stimuli were annotated for surprisal on Tortoise using `surprisal_german.py`.

```{r read-surprisals}
dat_gerpt2 <- read.csv('stone_surprisals_gerpt2.csv',
                       header = TRUE, sep = ";")
dat_gerpt2_large <- read.csv('stone_surprisals_gerpt2-large.csv',
                             header = TRUE, sep = ";")
dat_leo_7b <- read.csv('stone_surprisals_leo7b.csv',
                       header = TRUE, sep = ";")
dat_leo_13b <- read.csv('stone_surprisals_leo13b.csv',
                        header = TRUE, sep = ";")

dat_gerpt2 <- dat_gerpt2 %>% 
  dplyr::rename(surprisal_gerpt2 = surprisal)

dat_gerpt2_large <- dat_gerpt2_large %>% 
  dplyr::rename(surprisal_gerpt2_large = surprisal)

dat_leo_7b <- dat_leo_7b %>%
  dplyr::rename(surprisal_leo_7b = surprisal)

dat_leo_13b <- dat_leo_13b %>%
  dplyr::rename(surprisal_leo_13b = surprisal)

dat$surprisal_gerpt2 <- dat_gerpt2$surprisal_gerpt2
dat$surprisal_gerpt2_large <- dat_gerpt2_large$surprisal_gerpt2_large
dat$surprisal_leo_7b <- dat_leo_7b$surprisal_leo_7b
dat$surprisal_leo_13b <- dat_leo_13b$surprisal_leo_13b
```

Define functions to analyze surprisal:


```{r check-surprisal-functions}
correlation_plot <- function(dat, surprisal) {
  model <- gsub("^surprisal_", "", surprisal)
  p <- ggplot(dat, aes(x = cloze.target,
                       y = eval(parse(text = surprisal)),
                       color = cond_label)) +
    geom_point(size = 3) + 
    geom_text(aes(label = noun), vjust = -0.5, size = 4) +
    geom_smooth(method = "lm", formula = y ~ x, color = "darkred", se = TRUE) +
    labs(
      title = "Cloze-Surprisal Correlation",
      subtitle = model,
      x = "Target Cloze Probability",
      y = paste0("Target Surprisal (", model, ")"),
      color = "Condition"
    ) +
    scale_color_manual(values =  c("navy", "magenta3")) +
    theme_minimal()
  return(p)
}

get_surprisal_summary <- function(dat, surprisal) {
  dat %>% 
    dplyr::group_by(cond_label) %>% 
    dplyr::summarise(MeanSurprisal = mean(eval(parse(text = surprisal)),
                                          na.rm = TRUE),
                     SDSurprisal = sd(eval(parse(text = surprisal)),
                                      na.rm = TRUE),
                     MinSurprisal = min(eval(parse(text = surprisal)),
                                        na.rm = TRUE),
                     MaxSurprisal = max(eval(parse(text = surprisal)),
                                        na.rm = TRUE))
}

surprisal_density_plot <- function(dat,
                                   surprisal,
                                   plot_title = "Surprisal by condition") {
  model <- gsub("^surprisal_", "", surprisal)
  ggplot(data=dat,
         aes(eval(parse(text = surprisal)),
             fill = cond_label,
             color = cond_label)) + 
    geom_density(alpha = 0.6) + 
    scale_fill_manual(values = c("navy", "magenta3")) +
    scale_color_manual(values = c("navy", "magenta3"), guide = "none") +
    labs(title = plot_title,
         subtitle = model,
         x = paste0("Surprisal (", model, ")"),
         y = "Density",
         fill = "Condition") +
    theme_minimal()
}

surprisal_by_cond <- function(dat, surprisal) {
  dat_surprisal <- dat %>% 
    dplyr::group_by(item, cond_label) %>%
    dplyr::summarise(!!sym(surprisal) := mean(!!sym(surprisal)), .groups = "drop") %>%
    pivot_wider(names_from = cond_label, values_from = !!sym(surprisal))
  
  dat_strong <- dat_surprisal %>%
    filter(a < b)
  dat_strong <- dat %>% 
    filter(item %in% dat_strong$item)
  dat_weak <- dat_surprisal %>%
    filter(a >= b)
  dat_weak <- dat %>% 
    filter(item %in% dat_weak$item)
  
  cat("Checking surprisal by condition for model:",
      surprisal, end = "\n")
  cat("Number of items with surprisal in A < B:",
      length(unique(dat_strong$item)), end = "\n")
  cat("Number of items with surprisal in A >= B:",
      length(unique(dat_weak$item)), end = "\n")
  
  # Check item subset that fulfill the surprisal-condition criterion:
  cat("Mean surprisal by condition (only items with surprisal A <= B):",
      end = "\n")
  print(get_surprisal_summary(dat_strong, surprisal))
  
  # Density plot
  cat("Density plot (only items with surprisal A <= B):", end = "\n")
  surprisal_density_plot(dat_strong, surprisal,
                         plot_title = "Surprisal by condition (good items subset)")
} 
```


## Correlations

### Correlation between model surprisal values

Check correlations between the surprisal values produced by the different models:

```{r surprisal-correlations}
# cor(dat$surprisal_gerpt2, dat$surprisal_gerpt2_large)
# cor(dat$surprisal_gerpt2, dat$surprisal_leo_7b)
# cor(dat$surprisal_gerpt2, dat$surprisal_leo_13b)
# 
# cor(dat$surprisal_gerpt2_large, dat$surprisal_leo_7b)
# cor(dat$surprisal_gerpt2_large, dat$surprisal_leo_13b)
# 
# cor(dat$surprisal_leo_7b, dat$surprisal_leo_13b)

dat_cor <- dat %>%
  rename_with(~ str_remove(., "^surprisal_")) %>% 
  select(gerpt2, gerpt2_large, 
         leo_7b, leo_13b) %>% 
  cor() %>% 
  print()

corrplot(dat_cor, method = "color", type = "lower",
         col = colorRampPalette(c("blue", "white", "red"))(200), 
         tl.cex = 0.8, number.cex = 0.7, addCoef.col = "black", tl.srt = 45)
```

### Correlation between cloze and surprisal

Check correlations between surprisal and cloze values:

```{r surprisal-cloze-correlations}
dat_cor <- dat %>%
  select(cloze.target,
         surprisal_gerpt2, surprisal_gerpt2_large, 
         surprisal_leo_7b, surprisal_leo_13b) %>% 
  cor() %>% 
  as.data.frame() %>% 
  select(cloze.target) %>% 
  rownames_to_column(var = "model") %>%
  filter(model != "cloze.target") %>%
  mutate(model = str_remove(model, "^surprisal_")) %>% 
  mutate(model = factor(model, 
                        levels = c("gerpt2", "gerpt2_large", 
                                   "leo_7b", "leo_13b"))) %>% 
  print()

ggplot(dat_cor, aes(x = model,
                    y = cloze.target,
                    group = 1)) +
  geom_line(color = "steelblue") + 
  geom_point(color = "steelblue", size = 3) +
  geom_text(aes(label = round(cloze.target, 2)),
                vjust = -1, size = 4) +
  theme_minimal() +
  labs(title = "Surprisal-Cloze Correlations",
       x = "Model",
       y = "Correlation") +
  ylim(-1, 0) +                   # Set y-axis limits
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r plot-surprisal-cloze-correlations, fig.height = 20, fig.width = 20}
p1 <- correlation_plot(dat, "surprisal_gerpt2")
p2 <- correlation_plot(dat, "surprisal_gerpt2_large")
p3 <- correlation_plot(dat, "surprisal_leo_7b")
p4 <- correlation_plot(dat, "surprisal_leo_13b")
gridExtra::grid.arrange(p1, p2, p3, p4, nrow=2)
```


## Surprisal by condition & model

### GerPT2

```{r gerpt2-surprisal}
get_surprisal_summary(dat, "surprisal_gerpt2")
surprisal_density_plot(dat, "surprisal_gerpt2")
surprisal_by_cond(dat, "surprisal_gerpt2")
```

The full distributions are not very different between conditions.  
Manual inspection reveals that often surprisal is higher in condition a vs. b.
This is the case for 87 of the 224 items.  


### GerPT2-large

```{r gerpt2-large-surprisal}
get_surprisal_summary(dat, "surprisal_gerpt2_large")
surprisal_density_plot(dat, "surprisal_gerpt2_large")
surprisal_by_cond(dat, "surprisal_gerpt2_large")
```

GerPT2-large values lead to an improved alignment of surprisal with cloze values, but there are still 52 out of 224 items for which surprisal in condition A is larger or equal to that in condition B.


### Leo-7B

```{r leo_7b-surprisal}
get_surprisal_summary(dat, "surprisal_leo_7b")
surprisal_density_plot(dat, "surprisal_leo_7b")
surprisal_by_cond(dat, "surprisal_leo_7b")
```

For Leo-7B, only 23 items have higher/equal surprisal in condition A vs. B.

### Leo-13B

```{r leo_13b-surprisal}
get_surprisal_summary(dat, "surprisal_leo_13b")
surprisal_density_plot(dat, "surprisal_leo_13b")
surprisal_by_cond(dat, "surprisal_leo_13b")
```

For Leo-13B, this is further reduced to 22 items with surprisal A >= B.